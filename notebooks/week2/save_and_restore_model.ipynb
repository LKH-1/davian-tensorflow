{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save and Restore Model\n",
    "이번 튜토리얼에서는 variable들을 저장하고 다시 불러와서 학습을 재개해도록 하겠습니다. 아래 코드들은 week1 feed_forward_neural_network때와 거의 동일합니다. 마지막 아래 두 셀만 보시면 될 것 같습니다. 자세한 내용은 강의 시간에 설명하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./mnist/train-images-idx3-ubyte.gz\n",
      "Extracting ./mnist/train-labels-idx1-ubyte.gz\n",
      "Extracting ./mnist/t10k-images-idx3-ubyte.gz\n",
      "Extracting ./mnist/t10k-labels-idx1-ubyte.gz\n",
      "x_train:  (55000, 784)\n",
      "y_train:  (55000, 10)\n",
      "x_test:  (10000, 784)\n",
      "y_test:  (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "%matplotlib inline\n",
    "\n",
    "mnist = input_data.read_data_sets(\"./mnist\", one_hot=True)\n",
    "x_train = mnist.train.images\n",
    "y_train = mnist.train.labels\n",
    "x_test = mnist.test.images\n",
    "y_test = mnist.test.labels\n",
    "\n",
    "print \"x_train: \", x_train.shape\n",
    "print \"y_train: \", y_train.shape\n",
    "print \"x_test: \", x_test.shape\n",
    "print \"y_test: \", y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fully_connected(x, dim_in, dim_out, name):\n",
    "    with tf.variable_scope(name) as scope:\n",
    "        # create variables\n",
    "        w = tf.get_variable('w', shape=[dim_in, dim_out], \n",
    "                            initializer=tf.random_uniform_initializer(minval=-0.1, maxval=0.1))\n",
    "        b = tf.get_variable('b', shape=[dim_out])\n",
    "        \n",
    "        # create operations\n",
    "        out = tf.matmul(x, w) + b\n",
    "        \n",
    "        return out   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create model\n",
    "def neural_network(x, dim_in=784, dim_h=500, dim_out=10):\n",
    "    # 1st hidden layer with ReLU\n",
    "    h1 = fully_connected(x, dim_in, dim_h, name='h1')\n",
    "    h1 = tf.nn.relu(h1)\n",
    "    \n",
    "    # 2nd hidden layer with ReLU\n",
    "    h2 = fully_connected(h1, dim_h, dim_h, name='h2')\n",
    "    h2 = tf.nn.relu(h2)\n",
    "    \n",
    "    # output layer with linear\n",
    "    out = fully_connected(h2, dim_h, dim_out, name='out')\n",
    "    \n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, [None, 784])\n",
    "y = tf.placeholder(tf.float32, [None, 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Construct model with default value\n",
    "out = neural_network(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# loss and optimizer\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(out, y))\n",
    "optimizer = tf.train.RMSPropOptimizer(learning_rate=0.0001).minimize(loss)\n",
    "\n",
    "# Test model\n",
    "pred = tf.argmax(out, 1)\n",
    "target = tf.argmax(y, 1)\n",
    "correct_pred = tf.equal(pred, target)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 매 epoch마다 모든 variable들을 save합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.985\n",
      "Finished training!\n",
      "\n",
      "Test accuracy: 0.9065\n"
     ]
    }
   ],
   "source": [
    "batch_size = 100\n",
    "save_every = 1\n",
    "\n",
    "if not os.path.exists('model/'):\n",
    "    os.makedirs('model/')\n",
    "\n",
    "# launch the graph\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "with tf.Session(config=config) as sess:\n",
    "    # initialize tensor variables\n",
    "    tf.initialize_all_variables().run()\n",
    "    saver = tf.train.Saver(max_to_keep=20)\n",
    "    # training cycle\n",
    "    for epoch in range(1):\n",
    "        avg_loss = 0.\n",
    "        n_iters_per_epoch = int(mnist.train.num_examples / batch_size)\n",
    "        # loop over all batches\n",
    "        for i in range(n_iters_per_epoch):\n",
    "            x_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "            # run optimization op (backprop) and loss op (to get loss value)\n",
    "            _, c = sess.run([optimizer, loss], feed_dict={x: x_batch, y: y_batch})\n",
    "            # compute average loss\n",
    "            avg_loss += c / n_iters_per_epoch\n",
    "        print \"Epoch %d, Loss: %.3f\"% (epoch+1, avg_loss)\n",
    "        if epoch % save_every == 0:\n",
    "            saver.save(sess, save_path='model/fc', global_step=epoch+1)\n",
    "            \n",
    "    print \"Finished training!\"\n",
    "    print \"\\nTest accuracy:\", sess.run(accuracy, {x: mnist.test.images, y: mnist.test.labels})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save한 variable들을 다시 불러와 train을 재개합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.264\n",
      "Epoch 2, Loss: 0.195\n",
      "Epoch 3, Loss: 0.155\n",
      "Finished training!\n",
      "\n",
      "Test accuracy: 0.9563\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(config=config) as sess:\n",
    "    saver = tf.train.Saver()\n",
    "    saver.restore(sess, 'model/fc-1')\n",
    "    # training cycle\n",
    "    for epoch in range(3):\n",
    "        avg_loss = 0.\n",
    "        n_iters_per_epoch = int(mnist.train.num_examples / batch_size)\n",
    "        # loop over all batches\n",
    "        for i in range(n_iters_per_epoch):\n",
    "            x_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "            # run optimization op (backprop) and loss op (to get loss value)\n",
    "            _, c = sess.run([optimizer, loss], feed_dict={x: x_batch, y: y_batch})\n",
    "            # compute average loss\n",
    "            avg_loss += c / n_iters_per_epoch\n",
    "        print \"Epoch %d, Loss: %.3f\"% (epoch+1, avg_loss)\n",
    "        if epoch % save_every == 0:\n",
    "            saver.save(sess, save_path='model/fc', global_step=epoch+1)\n",
    "            \n",
    "    print \"Finished training!\"\n",
    "    print \"\\nTest accuracy:\", sess.run(accuracy, {x: mnist.test.images, y: mnist.test.labels})"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
