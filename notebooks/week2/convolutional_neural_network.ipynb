{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Convolutional Neural Networks\n",
    "이번 시간에는 간단하게 CNN을 구현해 보도록 하겠습니다. 이번 시간 역시 mnist를 가지고 하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./mnist/train-images-idx3-ubyte.gz\n",
      "Extracting ./mnist/train-labels-idx1-ubyte.gz\n",
      "Extracting ./mnist/t10k-images-idx3-ubyte.gz\n",
      "Extracting ./mnist/t10k-labels-idx1-ubyte.gz\n",
      "x_train:  (55000, 784)\n",
      "y_train:  (55000, 10)\n",
      "x_test:  (10000, 784)\n",
      "y_test:  (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "mnist = input_data.read_data_sets(\"./mnist\", one_hot=True)\n",
    "x_train = mnist.train.images\n",
    "y_train = mnist.train.labels\n",
    "x_test = mnist.test.images\n",
    "y_test = mnist.test.labels\n",
    "\n",
    "print \"x_train: \", x_train.shape\n",
    "print \"y_train: \", y_train.shape\n",
    "print \"x_test: \", x_test.shape\n",
    "print \"y_test: \", y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter setting \n",
    "\n",
    "이제 CNN에 들어갈 각종 parameter들을 설정합니다. 통상적으로 CNN에는 다음과 같은 parameter들이 필요합니다.\n",
    "- learning rate\n",
    "- batch size\n",
    "  * CNN을 학습할 때 가장 좋은 방법은 N개의 샘플이 있는 데이터를 한꺼번에 input으로 넣어서 학습하는 건데, training set이 50000개 이렇게 되면 메모리 문제가 발생할 수 있습니다. 그래서 할수없이 데이터를 가령 500개씩 분할(minibatch)로 나눠서 학습을 하게 됩니다. 이렇게 되면 50000개를 한번에 학습하는 대신, 500개만을 넣은 모델을 학습하고, 같은 모델을 다음 500개의 데이터로 다시 학습하는 걸 100번 하는 식으로 진행이 됩니다.\n",
    "- training iterations 혹은 number of epochs\n",
    "- input size \n",
    "  * 통상적으로 이미지를 행렬로 나타낼 때 [H(높이),W(너비),D(RGB)]로 나타냅니다. MNIST는 28x28의 흑백이미지이므로 input size는 28x28x1=784가 되고, CIFAR-10의 경우 32x32x10, imageNet은 224x224x3 등으로 해상도에 따라 달라집니다.\n",
    "- number of classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "learning_rate = 0.001\n",
    "training_iters = 200000\n",
    "batch_size = 100\n",
    "display_step = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Network parameters\n",
    "n_input = 784\n",
    "n_classes = 10\n",
    "dropout = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input & Output, 그리고 사용되는 dropout값을 담기 위한 placeholder를 지정합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Graph input\n",
    "x = tf.placeholder(tf.float32, [None, n_input])\n",
    "y = tf.placeholder(tf.float32, [None, n_classes])\n",
    "keep_prob = tf.placeholder(tf.float32) # to keep dropout probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CNN 내부에 사용되는 filter(=layer)(=weight) 및 bias들을 Variable의 형태로 생성합니다.\n",
    "- 얘네가 굳이 Variable인 이유는 CNN을 학습할 때 backpropagation을 계산하면서 값들을 매번 업데이트해줘야 하기 때문입니다.\n",
    "- weight랑 bias를 생성할 때, 내가 원하는 CNN이 어떻게 convolution 혹은 pooling을 하면서 input과 output matrix의 크기가 어떻게 달라지는지를 매번 계산해야 합니다. 마지막에 fully connected layer를 만들 때에는 계산이 복잡해질 수 있는데, 뒤에 이를 좀 더 쉽게 하는 법이 나옵니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "weights = {\n",
    "    # 5x5 conv, 1 input, 32 outputs\n",
    "    'wc1': tf.Variable(tf.random_normal([5, 5, 1, 32])),\n",
    "    # 5x5 conv, 32 inputs, 64 outputs\n",
    "    'wc2': tf.Variable(tf.random_normal([5, 5, 32, 64])),\n",
    "    # fully connected, 7*7*64 inputs, 1024 outputs\n",
    "    'wd1': tf.Variable(tf.random_normal([7*7*64, 1024])),\n",
    "    # 1024 inputs, 10 outputs (class prediction)\n",
    "    'out': tf.Variable(tf.random_normal([1024, n_classes]))\n",
    "}\n",
    "\n",
    "biases = {\n",
    "    'bc1': tf.Variable(tf.random_normal([32])),\n",
    "    'bc2': tf.Variable(tf.random_normal([64])),\n",
    "    'bd1': tf.Variable(tf.random_normal([1024])),\n",
    "    'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Network Implementation **\n",
    "\n",
    "이제 CNN architecture를 한번 구현해 보겠습니다. 우리 CNN은 다음과 같은 간단한 구조를 가집니다.\n",
    "\n",
    "1) 5x5 convolution (x32)\n",
    "\n",
    "2) ReLU\n",
    "\n",
    "3) Max pooling\n",
    "\n",
    "4) 5x5 convolution (x64)\n",
    "\n",
    "5) ReLU\n",
    "\n",
    "6) Max pooling\n",
    "\n",
    "7) Fully connected layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 모든 과정은 TF library 의 함수 6개만 알면 됩니다.\n",
    "\n",
    "1) tf.nn.conv2d(input, weights, strides, padding)\n",
    "- 용도: 원본 layer 부분부분마다 필터를 대고 2-D convolution을 진행해서 output layer를 만든다.\n",
    "* input: 원본 이미지의 placeholder 혹은 전 단계의 결과\n",
    "* weights: 아까 tf.Variable로 만들어놓은, 해당 단계에 써야 하는 weight\n",
    "* strides: [1,int,int,1]이나 대개 [1,1,1,1]로 합니다.\n",
    "* padding: 'SAME' 혹은 'VALID'가 있는데 거의 다 'SAME' 씁니다. ('SAME'이 zero padding 허용)\n",
    "\n",
    "2) tf.nn.max_pool(input, ksize, strides, padding)\n",
    "- 용도: window를 이미지에다 대고 해당 window 내 가장 큰 값만 반환하면서 축소된 layer를 만든다.\n",
    "* input: 원본 이미지의 placeholder 혹은 전 단계의 결과\n",
    "* ksize: [1,int,int,1], 여기서 int에는 1/int로 배율을 줄일 지를 나타냅니다.\n",
    "* strides: [1,int,int,1]이나 대개 ksize랑 똑같이 맞춥니다.\n",
    "* padding: 역시 'SAME'으로 맞춥시다. ('SAME'이 zero padding 허용)\n",
    "\n",
    "3) tf.nn.bias_add(input, bias_layer)\n",
    "- 용도: bias를 더해준다 (Wx+b의 b를 더해주는 겁니다)\n",
    "* input: 그냥 인풋\n",
    "* bias_layer: 앞서 weight와 함께 정의한 해당 bias입니다.\n",
    "\n",
    "4) tf.nn.relu(input)\n",
    "- 용도: nonlinearity를 만들어주기 위해, 가장 많이 쓰이는 ReLU 함수를 원본 layer에다 적용합니다.\n",
    "* input: 역시 그냥 인풋"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def CNN(x, weights, biases, dropout):\n",
    "    \n",
    "    # Reshape input picture\n",
    "    x = tf.reshape(x, shape=[-1, 28, 28, 1])\n",
    "    print \"Input x:               \", x.get_shape().as_list()\n",
    "\n",
    "    # 1st Convolution Layer\n",
    "    conv1 = tf.nn.conv2d(x, weights['wc1'], strides=[1,1,1,1], padding='SAME')\n",
    "    print \"After 1st conv:        \", conv1.get_shape().as_list()\n",
    "    conv1 = tf.nn.bias_add(conv1, biases['bc1'])\n",
    "    print \"After adding bias:     \", conv1.get_shape().as_list()\n",
    "    conv1 = tf.nn.relu(conv1)\n",
    "    print \"After ReLU:            \", conv1.get_shape().as_list()\n",
    "    # Max Pooling (down-sampling)\n",
    "    pool1 = tf.nn.max_pool(conv1, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')\n",
    "    print \"After 1st max_pooling: \", pool1.get_shape().as_list()\n",
    "\n",
    "    # 2nd Convolution Layer\n",
    "    conv2 = tf.nn.conv2d(pool1, weights['wc2'], strides=[1,1,1,1], padding='SAME')\n",
    "    print \"After 2nd conv:        \", conv2.get_shape().as_list()\n",
    "    conv2 = tf.nn.bias_add(conv2, biases['bc2'])\n",
    "    print \"After adding bias:     \", conv2.get_shape().as_list()\n",
    "    conv2 = tf.nn.relu(conv2)\n",
    "    print \"After ReLU:            \", conv2.get_shape().as_list()\n",
    "    # Max Pooling (down-sampling)\n",
    "    pool2 = tf.nn.max_pool(conv2, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')\n",
    "    print \"After 2nd max_pooling: \", pool2.get_shape().as_list()\n",
    "\n",
    "    # Fully connected layer\n",
    "    # Reshape conv2 output to fit fully connected layer input\n",
    "    fc1 = tf.reshape(pool2, [-1, weights['wd1'].get_shape().as_list()[0]])\n",
    "    fc1 = tf.add(tf.matmul(fc1, weights['wd1']), biases['bd1'])\n",
    "    print \"Fully connected:       \", fc1.get_shape().as_list()\n",
    "    fc1 = tf.nn.relu(fc1)\n",
    "    # Apply Dropout\n",
    "    fc1 = tf.nn.dropout(fc1, dropout)\n",
    "    # Output, class prediction\n",
    "    out = tf.add(tf.matmul(fc1, weights['out']), biases['out'])\n",
    "    print \"Output:                \", out.get_shape().as_list()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Optimization **\n",
    "\n",
    "이제 CNN 모델의 구조를 지었으니, 실제로 결과를 출력하게끔 해야죠.\n",
    "위의 def CNN이 결과물로 [None,10]을 뱉어냈는데, 이제 loss를 계산하고 optimize를 해야죠.\n",
    "구조는 다음과 같습니다.\n",
    "\n",
    "1) pred : 각 input을 넣었을 때 10개의 class에 대한 값들이 저장되어 있다\n",
    "  - a: [100,10]라고 하자\n",
    "  - a[0,0]=0.18, a[0,1]=0.02, a[0,2]=0.95, ..., a[0,9]=0.37 이런 식이면 a[0]이라는 샘플은 [2]에 해당하는 label, 즉 '1'일 거라고 예측\n",
    "  \n",
    "2) 예측한 pred와 실제 y를 받아서 softmax 계산을 하고 loss를 반환하는 cost. y 역시 [100,10]여야 한다.\n",
    "\n",
    "3) 그 cost를 인풋으로 받고, cost를 최대한으로 줄이기 위한 함수를 실행하는 optimizer를 생성한다.\n",
    "- optimizer들 종류는 다음과 같다. 아니, 더 많다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Construct model\n",
    "pred = CNN(x, weights, biases, keep_prob)\n",
    "# Define loss\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(pred, y))\n",
    "# Define optimizer\n",
    "#optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "# 다른 optimizer들도 한번 넣어보세요\n",
    "#optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "#optimizer = tf.train.AdadeltaOptimizer().minimize(cost)\n",
    "#optimizer = tf.train.AdagradOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "#optimizer = tf.train.MomentumOptimizer(learning_rate=learning_rate, momentum=0.5).minimize(cost)\n",
    "#optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "optimizer = tf.train.RMSPropOptimizer(learning_rate=learning_rate).minimize(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Performance metric **\n",
    "\n",
    "다음 함수들을 이용해서 accuracy를 측정할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Find correct prediction\n",
    "correct_pred = tf.equal(tf.argmax(pred,1), tf.argmax(y,1))\n",
    "# Get accuracy\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# initialize variables\n",
    "init = tf.initialize_all_variables()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "연구실에서 서버 돌릴 때 꿀팁, GPU 사용 관련 커맨드 알려드립니다\n",
    "\n",
    "1) 사용방법: tf.Session 혹은 tf.InteractiveSession의 argument로 해당 config 추가\n",
    "\n",
    "2) tf.ConfigProto 커맨드\n",
    "  - log_device_placement=True : 어느 device를 사용하는지를 알기 위한 device mapping입니다.\n",
    "  - allow_soft_placement=True : device를 설정했는데 해당 device가 없어서 (혹은 메모리가 다 차서) 안 돌아갈 \n",
    "    수 있는데, 이 커맨드를 통해 그런 문제를 방지할 수 있습니다.\n",
    "  - 제 경우에는 allow_soft_placement=True로 하니까, GPU로 지원되지 않는 단순 연산도 GPU가 하려고 하다 보니 발생하는 에러를 줄일 수 있었습니다. 사용범위를 유기적으로 조정하지 않을까 생각합니다.\n",
    "\n",
    "  ### tf.GPUOptions 커맨드\n",
    "      - (1) ** allow_growth=True ** : Tensorflow에서는 디폴트로 사용가능한 모든 메모리를 할당하는데, 이렇게 되면 mnist 하나 돌린다고 11기가씩 잡아먹을 수 있습니다. 이를 방지하기 위해 allow_growth=True로 하면 필요한 만큼의 메모리만 먹습니다.\n",
    "        - 11749MiB->461MiB\n",
    "      - (2) ** per_process_gpu_memory_fraction=0.1 ** : 역시 이 커맨드를 켜면 GPU 최대 메모리 중 0.1, 즉 10%만을\n",
    "      사용한다는 뜻입니다. 돌려야 하는 네트워크가 allow_growth로도 많은 메모리를 잡아먹게 되면 이 커맨드를\n",
    "      통해 강제할당할 수 있습니다. 필요에 맞게 퍼센트를 조절할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gpu_options = tf.GPUOptions(allow_growth=True, per_process_gpu_memory_fraction=0.1)\n",
    "config = tf.ConfigProto(log_device_placement=True, allow_soft_placement=True,\n",
    "                        gpu_options=gpu_options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# apply config when starting session\n",
    "sess = tf.InteractiveSession(config=config)\n",
    "init.run()\n",
    "step = 1\n",
    "\n",
    "while step * batch_size < training_iters:\n",
    "    batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "    # Run optimization\n",
    "    sess.run(optimizer, feed_dict={x: batch_x, y: batch_y, keep_prob: dropout})\n",
    "    if step % display_step == 0:\n",
    "    # Calculate batch loss and accuracy\n",
    "        loss, acc = sess.run([cost, accuracy], feed_dict={x: batch_x,\n",
    "                                                              y: batch_y,\n",
    "                                                              keep_prob: 1.})\n",
    "        print(\"Iter \" + str(step*batch_size) + \", Minibatch Loss= \" + \\\n",
    "                  \"{:.6f}\".format(loss) + \", Training Accuracy= \" + \\\n",
    "                  \"{:.5f}\".format(acc))\n",
    "    step += 1\n",
    "print(\"Optimization Finished!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "    # Calculate accuracy for 256 mnist test images\n",
    "    print(\"Testing Accuracy:\", \\\n",
    "        sess.run(accuracy, feed_dict={x: mnist.test.images[:256],\n",
    "                                      y: mnist.test.labels[:256],\n",
    "                                      keep_prob: 1.}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "편의를 위해 weight, bias를 따로 설정하지 않는 방법도 있습니다. 지난 시간 때 배운 tf.get_variable()을 이용하면 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def conv2d_simple(x, filter_size, dim_in, dim_out, name, strides=1):\n",
    "    with tf.variable_scope(name) as scope:\n",
    "        w = tf.get_variable('w', shape=[filter_size, filter_size, dim_in, dim_out],\n",
    "                initializer=tf.random_uniform_initializer(minval=-0.1, maxval=0.1))\n",
    "        b = tf.get_variable('b', shape=[dim_out])\n",
    "        x = tf.nn.conv2d(x,w,strides=[1,strides,strides,1],padding='SAME')\n",
    "        x = tf.nn.bias_add(x,b)\n",
    "        return tf.nn.relu(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def fc_simple(x,dim_fc,n_classes,name):\n",
    "    with tf.variable_scope(name) as scope:\n",
    "        size_arr = x.get_shape().as_list()\n",
    "        print type(size_arr)\n",
    "        print size_arr[1]\n",
    "        dim_in = np.prod(np.asarray(size_arr[1:]))\n",
    "        print dim_in\n",
    "        print x.get_shape().as_list()\n",
    "        x = tf.reshape(x, [-1,dim_in])\n",
    "        dim_in = x.get_shape().as_list()[1]\n",
    "        print dim_in\n",
    "        wd = tf.get_variable('wd', shape=[dim_in, dim_fc],\n",
    "                initializer=tf.random_uniform_initializer(minval=-0.1, maxval=0.1))\n",
    "        bd = tf.get_variable('bd', shape=[dim_fc])\n",
    "        wout = tf.get_variable('wout', shape=[dim_fc, n_classes],\n",
    "                initializer=tf.random_uniform_initializer(minval=-0.1, maxval=0.1))\n",
    "        bout = tf.get_variable('bout', shape=[n_classes])\n",
    "        fc1 = tf.add(tf.matmul(x, wd),bd)\n",
    "        fc1 = tf.nn.relu(fc1)\n",
    "        fc1 = tf.nn.dropout(fc1,dropout)\n",
    "        \n",
    "        out = tf.add(tf.matmul(fc1, wout),bout)\n",
    "        return out "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def maxpool2d(x, k=2):\n",
    "    return tf.nn.max_pool(x, ksize=[1,k,k,1], strides=[1,k,k,1], padding='SAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def CNN_simple(x, dropout):\n",
    "    \n",
    "    # Reshape input picture\n",
    "    x = tf.reshape(x, shape=[-1, 28, 28, 1])\n",
    "    print \"x shape\", x.get_shape().as_list\n",
    "    # 1st Convolution Layer\n",
    "    conv1 = conv2d_simple(x, 5, 1, 32, 'conv1')\n",
    "    print \"After 1st conv:        \", conv1.get_shape().as_list()\n",
    "    # Max Pooling (down-sampling)\n",
    "    conv2 = maxpool2d(conv1, k=2)\n",
    "    print \"After 1st max_pooling: \", conv1.get_shape().as_list()\n",
    "\n",
    "    # 2nd Convolution Layer\n",
    "    conv3 = conv2d_simple(conv2, 5, 32, 64, 'conv2')\n",
    "    print \"After 2nd conv:        \", conv2.get_shape().as_list()\n",
    "    # Max Pooling (down-sampling)\n",
    "    conv4 = maxpool2d(conv3, k=2)\n",
    "    print \"After 2nd max_pooling: \", conv2.get_shape().as_list()\n",
    "\n",
    "    # Fully connected layer\n",
    "    out = fc_simple(conv4, 1024, n_classes, 'fc1')\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def VGGNet_simple(x, dropout):\n",
    "    \n",
    "    # Reshape input picture\n",
    "    x = tf.reshape(x, shape=[-1, 224, 224, 3])\n",
    "    print \"x shape\", x.get_shape().as_list\n",
    "    conv1 = conv2d_simple(x, 3, 3, 64, 'conv1')\n",
    "    print \"After 1st conv:        \", conv1.get_shape().as_list()\n",
    "    conv2 = conv2d_simple(x, 3, 64, 64, 'conv2')\n",
    "    print \"After 2nd conv:        \", conv1.get_shape().as_list()\n",
    "    # Max Pooling (down-sampling)\n",
    "    conv2 = maxpool2d(conv1, k=2)\n",
    "    print \"After 1st max_pooling: \", conv1.get_shape().as_list()\n",
    "\n",
    "    # 2nd Convolution Layer\n",
    "    conv3 = conv2d_simple(conv2, 5, 32, 64, 'conv2')\n",
    "    print \"After 2nd conv:        \", conv2.get_shape().as_list()\n",
    "    # Max Pooling (down-sampling)\n",
    "    conv4 = maxpool2d(conv3, k=2)\n",
    "    print \"After 2nd max_pooling: \", conv2.get_shape().as_list()\n",
    "\n",
    "    # Fully connected layer\n",
    "    out = fc_simple(conv4, 1024, n_classes, 'fc1')\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x shape <bound method TensorShape.as_list of TensorShape([Dimension(None), Dimension(28), Dimension(28), Dimension(1)])>\n",
      "After 1st conv:         [None, 28, 28, 32]\n",
      "After 1st max_pooling:  [None, 28, 28, 32]\n",
      "After 2nd conv:         [None, 14, 14, 32]\n",
      "After 2nd max_pooling:  [None, 14, 14, 32]\n",
      "<type 'list'>\n",
      "7\n",
      "3136\n",
      "[None, 7, 7, 64]\n",
      "3136\n"
     ]
    }
   ],
   "source": [
    "#tf.get_variable_scope().reuse_variables()\n",
    "# Construct model\n",
    "pred = CNN_simple(x, keep_prob)\n",
    "# Define loss\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(pred, y))\n",
    "# Define optimizer\n",
    "#optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "# 다른 optimizer들도 한번 트라이 해보세요\n",
    "#optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "#optimizer = tf.train.AdadeltaOptimizer().minimize(cost)\n",
    "#optimizer = tf.train.AdagradOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "#optimizer = tf.train.MomentumOptimizer(learning_rate=learning_rate, momentum=0.5).minimize(cost)\n",
    "#optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "optimizer = tf.train.RMSPropOptimizer(learning_rate=learning_rate).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Find correct prediction\n",
    "correct_pred = tf.equal(tf.argmax(pred,1), tf.argmax(y,1))\n",
    "# Get accuracy\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "# initialize variables\n",
    "init = tf.initialize_all_variables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gpu_options = tf.GPUOptions(allow_growth=True, per_process_gpu_memory_fraction=0.333)\n",
    "config = tf.ConfigProto(log_device_placement=True, allow_soft_placement=True,\n",
    "                        gpu_options=gpu_options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 1000, Minibatch Loss= 5.156737, Training Accuracy= 0.16000\n",
      "Iter 2000, Minibatch Loss= 3.788680, Training Accuracy= 0.07000\n",
      "Iter 3000, Minibatch Loss= 2.893919, Training Accuracy= 0.14000\n",
      "Iter 4000, Minibatch Loss= 2.974441, Training Accuracy= 0.16000\n",
      "Iter 5000, Minibatch Loss= 2.865923, Training Accuracy= 0.11000\n",
      "Iter 6000, Minibatch Loss= 2.589398, Training Accuracy= 0.16000\n",
      "Iter 7000, Minibatch Loss= 2.277774, Training Accuracy= 0.18000\n",
      "Iter 8000, Minibatch Loss= 2.523856, Training Accuracy= 0.25000\n",
      "Iter 9000, Minibatch Loss= 1.936950, Training Accuracy= 0.33000\n",
      "Iter 10000, Minibatch Loss= 1.393814, Training Accuracy= 0.52000\n",
      "Iter 11000, Minibatch Loss= 0.833924, Training Accuracy= 0.75000\n",
      "Iter 12000, Minibatch Loss= 0.840184, Training Accuracy= 0.73000\n",
      "Iter 13000, Minibatch Loss= 0.434115, Training Accuracy= 0.85000\n",
      "Iter 14000, Minibatch Loss= 0.328565, Training Accuracy= 0.85000\n",
      "Iter 15000, Minibatch Loss= 0.401292, Training Accuracy= 0.86000\n",
      "Iter 16000, Minibatch Loss= 0.536589, Training Accuracy= 0.83000\n",
      "Iter 17000, Minibatch Loss= 0.238929, Training Accuracy= 0.94000\n",
      "Iter 18000, Minibatch Loss= 0.142983, Training Accuracy= 0.97000\n",
      "Iter 19000, Minibatch Loss= 0.366431, Training Accuracy= 0.88000\n",
      "Iter 20000, Minibatch Loss= 0.253159, Training Accuracy= 0.92000\n",
      "Iter 21000, Minibatch Loss= 0.165009, Training Accuracy= 0.95000\n",
      "Iter 22000, Minibatch Loss= 0.157794, Training Accuracy= 0.94000\n",
      "Iter 23000, Minibatch Loss= 0.132249, Training Accuracy= 0.96000\n",
      "Iter 24000, Minibatch Loss= 0.179352, Training Accuracy= 0.92000\n",
      "Iter 25000, Minibatch Loss= 0.257513, Training Accuracy= 0.90000\n",
      "Iter 26000, Minibatch Loss= 0.225014, Training Accuracy= 0.93000\n",
      "Iter 27000, Minibatch Loss= 0.138460, Training Accuracy= 0.95000\n",
      "Iter 28000, Minibatch Loss= 0.052405, Training Accuracy= 0.99000\n",
      "Iter 29000, Minibatch Loss= 0.067096, Training Accuracy= 0.99000\n",
      "Iter 30000, Minibatch Loss= 0.100283, Training Accuracy= 0.98000\n",
      "Iter 31000, Minibatch Loss= 0.093455, Training Accuracy= 0.96000\n",
      "Iter 32000, Minibatch Loss= 0.062660, Training Accuracy= 0.98000\n",
      "Iter 33000, Minibatch Loss= 0.091543, Training Accuracy= 0.96000\n",
      "Iter 34000, Minibatch Loss= 0.065552, Training Accuracy= 0.98000\n",
      "Iter 35000, Minibatch Loss= 0.078010, Training Accuracy= 0.98000\n",
      "Iter 36000, Minibatch Loss= 0.071407, Training Accuracy= 0.98000\n",
      "Iter 37000, Minibatch Loss= 0.179591, Training Accuracy= 0.95000\n",
      "Iter 38000, Minibatch Loss= 0.125196, Training Accuracy= 0.96000\n",
      "Iter 39000, Minibatch Loss= 0.149922, Training Accuracy= 0.97000\n",
      "Iter 40000, Minibatch Loss= 0.133593, Training Accuracy= 0.95000\n",
      "Iter 41000, Minibatch Loss= 0.081330, Training Accuracy= 0.96000\n",
      "Iter 42000, Minibatch Loss= 0.046728, Training Accuracy= 0.98000\n",
      "Iter 43000, Minibatch Loss= 0.089017, Training Accuracy= 0.96000\n",
      "Iter 44000, Minibatch Loss= 0.171299, Training Accuracy= 0.93000\n",
      "Iter 45000, Minibatch Loss= 0.048378, Training Accuracy= 0.98000\n",
      "Iter 46000, Minibatch Loss= 0.033984, Training Accuracy= 0.99000\n",
      "Iter 47000, Minibatch Loss= 0.139876, Training Accuracy= 0.95000\n",
      "Iter 48000, Minibatch Loss= 0.164270, Training Accuracy= 0.96000\n",
      "Iter 49000, Minibatch Loss= 0.070118, Training Accuracy= 0.97000\n",
      "Iter 50000, Minibatch Loss= 0.104051, Training Accuracy= 0.96000\n",
      "Iter 51000, Minibatch Loss= 0.033467, Training Accuracy= 0.98000\n",
      "Iter 52000, Minibatch Loss= 0.010657, Training Accuracy= 1.00000\n",
      "Iter 53000, Minibatch Loss= 0.022939, Training Accuracy= 0.99000\n",
      "Iter 54000, Minibatch Loss= 0.012940, Training Accuracy= 0.99000\n",
      "Iter 55000, Minibatch Loss= 0.232273, Training Accuracy= 0.98000\n",
      "Iter 56000, Minibatch Loss= 0.014402, Training Accuracy= 1.00000\n",
      "Iter 57000, Minibatch Loss= 0.032918, Training Accuracy= 0.99000\n",
      "Iter 58000, Minibatch Loss= 0.019355, Training Accuracy= 1.00000\n",
      "Iter 59000, Minibatch Loss= 0.020403, Training Accuracy= 1.00000\n",
      "Iter 60000, Minibatch Loss= 0.039931, Training Accuracy= 0.99000\n",
      "Iter 61000, Minibatch Loss= 0.023643, Training Accuracy= 1.00000\n",
      "Iter 62000, Minibatch Loss= 0.052079, Training Accuracy= 0.97000\n",
      "Iter 63000, Minibatch Loss= 0.010503, Training Accuracy= 1.00000\n",
      "Iter 64000, Minibatch Loss= 0.028517, Training Accuracy= 0.99000\n",
      "Iter 65000, Minibatch Loss= 0.088100, Training Accuracy= 0.96000\n",
      "Iter 66000, Minibatch Loss= 0.051845, Training Accuracy= 0.98000\n",
      "Iter 67000, Minibatch Loss= 0.012455, Training Accuracy= 1.00000\n",
      "Iter 68000, Minibatch Loss= 0.104454, Training Accuracy= 0.98000\n",
      "Iter 69000, Minibatch Loss= 0.017547, Training Accuracy= 0.99000\n",
      "Iter 70000, Minibatch Loss= 0.133570, Training Accuracy= 0.97000\n",
      "Iter 71000, Minibatch Loss= 0.073346, Training Accuracy= 0.97000\n",
      "Iter 72000, Minibatch Loss= 0.014905, Training Accuracy= 0.99000\n",
      "Iter 73000, Minibatch Loss= 0.235635, Training Accuracy= 0.99000\n",
      "Iter 74000, Minibatch Loss= 0.067588, Training Accuracy= 0.98000\n",
      "Iter 75000, Minibatch Loss= 0.029758, Training Accuracy= 0.98000\n",
      "Iter 76000, Minibatch Loss= 0.012884, Training Accuracy= 1.00000\n",
      "Iter 77000, Minibatch Loss= 0.017350, Training Accuracy= 1.00000\n",
      "Iter 78000, Minibatch Loss= 0.042957, Training Accuracy= 0.99000\n",
      "Iter 79000, Minibatch Loss= 0.012335, Training Accuracy= 1.00000\n",
      "Iter 80000, Minibatch Loss= 0.006225, Training Accuracy= 1.00000\n",
      "Iter 81000, Minibatch Loss= 0.048539, Training Accuracy= 0.98000\n",
      "Iter 82000, Minibatch Loss= 0.007030, Training Accuracy= 1.00000\n",
      "Iter 83000, Minibatch Loss= 0.040950, Training Accuracy= 0.98000\n",
      "Iter 84000, Minibatch Loss= 0.021686, Training Accuracy= 1.00000\n",
      "Iter 85000, Minibatch Loss= 0.014096, Training Accuracy= 1.00000\n",
      "Iter 86000, Minibatch Loss= 0.096678, Training Accuracy= 0.98000\n",
      "Iter 87000, Minibatch Loss= 0.020426, Training Accuracy= 0.99000\n",
      "Iter 88000, Minibatch Loss= 0.051442, Training Accuracy= 0.99000\n",
      "Iter 89000, Minibatch Loss= 0.035534, Training Accuracy= 0.99000\n",
      "Iter 90000, Minibatch Loss= 0.013591, Training Accuracy= 0.99000\n",
      "Iter 91000, Minibatch Loss= 0.021840, Training Accuracy= 0.99000\n",
      "Iter 92000, Minibatch Loss= 0.008318, Training Accuracy= 1.00000\n",
      "Iter 93000, Minibatch Loss= 0.080072, Training Accuracy= 0.97000\n",
      "Iter 94000, Minibatch Loss= 0.063187, Training Accuracy= 0.98000\n",
      "Iter 95000, Minibatch Loss= 0.034742, Training Accuracy= 1.00000\n",
      "Iter 96000, Minibatch Loss= 0.010722, Training Accuracy= 1.00000\n",
      "Iter 97000, Minibatch Loss= 0.095592, Training Accuracy= 0.98000\n",
      "Iter 98000, Minibatch Loss= 0.049110, Training Accuracy= 0.98000\n",
      "Iter 99000, Minibatch Loss= 0.037599, Training Accuracy= 0.99000\n",
      "Iter 100000, Minibatch Loss= 0.014326, Training Accuracy= 1.00000\n",
      "Iter 101000, Minibatch Loss= 0.009117, Training Accuracy= 1.00000\n",
      "Iter 102000, Minibatch Loss= 0.036471, Training Accuracy= 0.99000\n",
      "Iter 103000, Minibatch Loss= 0.110719, Training Accuracy= 0.97000\n",
      "Iter 104000, Minibatch Loss= 0.002623, Training Accuracy= 1.00000\n",
      "Iter 105000, Minibatch Loss= 0.022343, Training Accuracy= 0.99000\n",
      "Iter 106000, Minibatch Loss= 0.050177, Training Accuracy= 0.98000\n",
      "Iter 107000, Minibatch Loss= 0.025437, Training Accuracy= 1.00000\n",
      "Iter 108000, Minibatch Loss= 0.005084, Training Accuracy= 1.00000\n",
      "Iter 109000, Minibatch Loss= 0.007151, Training Accuracy= 1.00000\n",
      "Iter 110000, Minibatch Loss= 0.017300, Training Accuracy= 1.00000\n",
      "Iter 111000, Minibatch Loss= 0.020237, Training Accuracy= 1.00000\n",
      "Iter 112000, Minibatch Loss= 0.014929, Training Accuracy= 1.00000\n",
      "Iter 113000, Minibatch Loss= 0.041992, Training Accuracy= 0.99000\n",
      "Iter 114000, Minibatch Loss= 0.002744, Training Accuracy= 1.00000\n",
      "Iter 115000, Minibatch Loss= 0.099505, Training Accuracy= 0.98000\n",
      "Iter 116000, Minibatch Loss= 0.006663, Training Accuracy= 1.00000\n",
      "Iter 117000, Minibatch Loss= 0.017723, Training Accuracy= 1.00000\n",
      "Iter 118000, Minibatch Loss= 0.034394, Training Accuracy= 0.98000\n",
      "Iter 119000, Minibatch Loss= 0.072813, Training Accuracy= 0.98000\n",
      "Iter 120000, Minibatch Loss= 0.015973, Training Accuracy= 1.00000\n",
      "Iter 121000, Minibatch Loss= 0.037423, Training Accuracy= 0.98000\n",
      "Iter 122000, Minibatch Loss= 0.008278, Training Accuracy= 1.00000\n",
      "Iter 123000, Minibatch Loss= 0.013017, Training Accuracy= 1.00000\n",
      "Iter 124000, Minibatch Loss= 0.008026, Training Accuracy= 1.00000\n",
      "Iter 125000, Minibatch Loss= 0.012660, Training Accuracy= 1.00000\n",
      "Iter 126000, Minibatch Loss= 0.016892, Training Accuracy= 1.00000\n",
      "Iter 127000, Minibatch Loss= 0.008476, Training Accuracy= 1.00000\n",
      "Iter 128000, Minibatch Loss= 0.004752, Training Accuracy= 1.00000\n",
      "Iter 129000, Minibatch Loss= 0.041019, Training Accuracy= 0.98000\n",
      "Iter 130000, Minibatch Loss= 0.029361, Training Accuracy= 0.99000\n",
      "Iter 131000, Minibatch Loss= 0.013000, Training Accuracy= 1.00000\n",
      "Iter 132000, Minibatch Loss= 0.001853, Training Accuracy= 1.00000\n",
      "Iter 133000, Minibatch Loss= 0.042249, Training Accuracy= 0.99000\n",
      "Iter 134000, Minibatch Loss= 0.009861, Training Accuracy= 1.00000\n",
      "Iter 135000, Minibatch Loss= 0.086754, Training Accuracy= 0.97000\n",
      "Iter 136000, Minibatch Loss= 0.006734, Training Accuracy= 1.00000\n",
      "Iter 137000, Minibatch Loss= 0.013113, Training Accuracy= 1.00000\n",
      "Iter 138000, Minibatch Loss= 0.002328, Training Accuracy= 1.00000\n",
      "Iter 139000, Minibatch Loss= 0.018070, Training Accuracy= 0.99000\n",
      "Iter 140000, Minibatch Loss= 0.041927, Training Accuracy= 0.98000\n",
      "Iter 141000, Minibatch Loss= 0.008542, Training Accuracy= 1.00000\n",
      "Iter 142000, Minibatch Loss= 0.020384, Training Accuracy= 0.99000\n",
      "Iter 143000, Minibatch Loss= 0.024727, Training Accuracy= 0.99000\n",
      "Iter 144000, Minibatch Loss= 0.031304, Training Accuracy= 0.98000\n",
      "Iter 145000, Minibatch Loss= 0.008898, Training Accuracy= 1.00000\n",
      "Iter 146000, Minibatch Loss= 0.021005, Training Accuracy= 0.99000\n",
      "Iter 147000, Minibatch Loss= 0.014093, Training Accuracy= 1.00000\n",
      "Iter 148000, Minibatch Loss= 0.032043, Training Accuracy= 0.99000\n",
      "Iter 149000, Minibatch Loss= 0.048852, Training Accuracy= 0.99000\n",
      "Iter 150000, Minibatch Loss= 0.046352, Training Accuracy= 0.99000\n",
      "Iter 151000, Minibatch Loss= 0.019635, Training Accuracy= 0.99000\n",
      "Iter 152000, Minibatch Loss= 0.016086, Training Accuracy= 1.00000\n",
      "Iter 153000, Minibatch Loss= 0.019738, Training Accuracy= 0.99000\n",
      "Iter 154000, Minibatch Loss= 0.022177, Training Accuracy= 0.99000\n",
      "Iter 155000, Minibatch Loss= 0.007054, Training Accuracy= 1.00000\n",
      "Iter 156000, Minibatch Loss= 0.007265, Training Accuracy= 1.00000\n",
      "Iter 157000, Minibatch Loss= 0.004764, Training Accuracy= 1.00000\n",
      "Iter 158000, Minibatch Loss= 0.051481, Training Accuracy= 0.99000\n",
      "Iter 159000, Minibatch Loss= 0.010767, Training Accuracy= 1.00000\n",
      "Iter 160000, Minibatch Loss= 0.011972, Training Accuracy= 1.00000\n",
      "Iter 161000, Minibatch Loss= 0.032038, Training Accuracy= 0.97000\n",
      "Iter 162000, Minibatch Loss= 0.010976, Training Accuracy= 1.00000\n",
      "Iter 163000, Minibatch Loss= 0.014842, Training Accuracy= 1.00000\n",
      "Iter 164000, Minibatch Loss= 0.012277, Training Accuracy= 1.00000\n",
      "Iter 165000, Minibatch Loss= 0.010358, Training Accuracy= 1.00000\n",
      "Iter 166000, Minibatch Loss= 0.004913, Training Accuracy= 1.00000\n",
      "Iter 167000, Minibatch Loss= 0.001013, Training Accuracy= 1.00000\n",
      "Iter 168000, Minibatch Loss= 0.017281, Training Accuracy= 0.99000\n",
      "Iter 169000, Minibatch Loss= 0.035327, Training Accuracy= 0.97000\n",
      "Iter 170000, Minibatch Loss= 0.007448, Training Accuracy= 1.00000\n",
      "Iter 171000, Minibatch Loss= 0.001881, Training Accuracy= 1.00000\n",
      "Iter 172000, Minibatch Loss= 0.003468, Training Accuracy= 1.00000\n",
      "Iter 173000, Minibatch Loss= 0.006349, Training Accuracy= 1.00000\n",
      "Iter 174000, Minibatch Loss= 0.008405, Training Accuracy= 1.00000\n",
      "Iter 175000, Minibatch Loss= 0.011470, Training Accuracy= 1.00000\n",
      "Iter 176000, Minibatch Loss= 0.005612, Training Accuracy= 1.00000\n",
      "Iter 177000, Minibatch Loss= 0.044892, Training Accuracy= 0.99000\n",
      "Iter 178000, Minibatch Loss= 0.009110, Training Accuracy= 1.00000\n",
      "Iter 179000, Minibatch Loss= 0.036447, Training Accuracy= 0.99000\n",
      "Iter 180000, Minibatch Loss= 0.040204, Training Accuracy= 0.99000\n",
      "Iter 181000, Minibatch Loss= 0.003044, Training Accuracy= 1.00000\n",
      "Iter 182000, Minibatch Loss= 0.006971, Training Accuracy= 1.00000\n",
      "Iter 183000, Minibatch Loss= 0.012116, Training Accuracy= 1.00000\n",
      "Iter 184000, Minibatch Loss= 0.019941, Training Accuracy= 0.99000\n",
      "Iter 185000, Minibatch Loss= 0.005298, Training Accuracy= 1.00000\n",
      "Iter 186000, Minibatch Loss= 0.002918, Training Accuracy= 1.00000\n",
      "Iter 187000, Minibatch Loss= 0.025179, Training Accuracy= 0.99000\n",
      "Iter 188000, Minibatch Loss= 0.018644, Training Accuracy= 0.99000\n",
      "Iter 189000, Minibatch Loss= 0.001919, Training Accuracy= 1.00000\n",
      "Iter 190000, Minibatch Loss= 0.018139, Training Accuracy= 0.99000\n",
      "Iter 191000, Minibatch Loss= 0.036237, Training Accuracy= 0.98000\n",
      "Iter 192000, Minibatch Loss= 0.006367, Training Accuracy= 1.00000\n",
      "Iter 193000, Minibatch Loss= 0.010147, Training Accuracy= 1.00000\n",
      "Iter 194000, Minibatch Loss= 0.008115, Training Accuracy= 1.00000\n",
      "Iter 195000, Minibatch Loss= 0.018687, Training Accuracy= 0.99000\n",
      "Iter 196000, Minibatch Loss= 0.006858, Training Accuracy= 1.00000\n",
      "Iter 197000, Minibatch Loss= 0.008350, Training Accuracy= 1.00000\n",
      "Iter 198000, Minibatch Loss= 0.005463, Training Accuracy= 1.00000\n",
      "Iter 199000, Minibatch Loss= 0.006412, Training Accuracy= 1.00000\n",
      "Optimization Finished!\n"
     ]
    }
   ],
   "source": [
    "# apply config when starting session\n",
    "sess = tf.InteractiveSession(config=config)\n",
    "init.run()\n",
    "step = 1\n",
    "\n",
    "while step * batch_size < training_iters:\n",
    "    batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "    # Run optimization\n",
    "    sess.run(optimizer, feed_dict={x: batch_x, y: batch_y, keep_prob: dropout})\n",
    "    if step % display_step == 0:\n",
    "    # Calculate batch loss and accuracy\n",
    "        loss, acc = sess.run([cost, accuracy], feed_dict={x: batch_x,\n",
    "                                                              y: batch_y,\n",
    "                                                              keep_prob: 1.})\n",
    "        print(\"Iter \" + str(step*batch_size) + \", Minibatch Loss= \" + \\\n",
    "                  \"{:.6f}\".format(loss) + \", Training Accuracy= \" + \\\n",
    "                  \"{:.5f}\".format(acc))\n",
    "    step += 1\n",
    "print(\"Optimization Finished!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Testing Accuracy:', 0.9765625)\n"
     ]
    }
   ],
   "source": [
    "    # Calculate accuracy for 256 mnist test images\n",
    "    print(\"Testing Accuracy:\", \\\n",
    "        sess.run(accuracy, feed_dict={x: mnist.test.images[:256],\n",
    "                                      y: mnist.test.labels[:256],\n",
    "                                      keep_prob: 1.}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
