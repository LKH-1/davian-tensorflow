{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorBoard\n",
    "In this tutorial, we are going to implement a neural network with fully-connected layers to perform classification, visualize the model and plot the loss and gradients by using a tensorboard.\n",
    "\n",
    "![alt text](jpg/tensorboard.jpg \"model image\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./mnist/train-images-idx3-ubyte.gz\n",
      "Extracting ./mnist/train-labels-idx1-ubyte.gz\n",
      "Extracting ./mnist/t10k-images-idx3-ubyte.gz\n",
      "Extracting ./mnist/t10k-labels-idx1-ubyte.gz\n",
      "x_train:  (55000, 784)\n",
      "y_train:  (55000, 10)\n",
      "x_test:  (10000, 784)\n",
      "y_test:  (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "\n",
    "mnist = input_data.read_data_sets(\"./mnist\", one_hot=True)\n",
    "x_train = mnist.train.images\n",
    "y_train = mnist.train.labels\n",
    "x_test = mnist.test.images\n",
    "y_test = mnist.test.labels\n",
    "\n",
    "print \"x_train: \", x_train.shape\n",
    "print \"y_train: \", y_train.shape\n",
    "print \"x_test: \", x_test.shape\n",
    "print \"y_test: \", y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fully-connected layer\n",
    "To implement a fully-connected layer, we can simply use `tf.matmul` function for 2D matrix multiplication. In this code, we are using `tf.variable_scope` and `tf.get_variable` functions to encapsulate and manage tensor variables effectively. Also, We are choosing a `tf.random_uniform_initializer` as variable initializer for our model. There are some other variable initializers such as `tf.random_normal_initializer and tf.truncated_normal_initializer`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fully_connected(x, dim_in, dim_out, name):\n",
    "    with tf.variable_scope(name) as scope:\n",
    "        # create variables\n",
    "        w = tf.get_variable('w', shape=[dim_in, dim_out], \n",
    "                            initializer=tf.random_uniform_initializer(minval=-0.1, maxval=0.1))\n",
    "        b = tf.get_variable('b', shape=[dim_out])\n",
    "        \n",
    "        # create operations\n",
    "        out = tf.matmul(x, w) + b\n",
    "        \n",
    "        return out    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural network\n",
    "Now, we will develope a neural network with 2 hidden layers using a `fully_connected` function. In this code, we are using a `tf.nn.relu` as our non-linear activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create model\n",
    "def neural_network(x, dim_in=784, dim_h=500, dim_out=10):\n",
    "    # 1st hidden layer with ReLU\n",
    "    h1 = fully_connected(x, dim_in, dim_h, name='hidden_layer_1')\n",
    "    h1 = tf.nn.relu(h1)\n",
    "    \n",
    "    # 2nd hidden layer with ReLU\n",
    "    h2 = fully_connected(h1, dim_h, dim_h, name='hidden_layer_2')\n",
    "    h2 = tf.nn.relu(h2)\n",
    "    \n",
    "    # output layer with linear\n",
    "    out = fully_connected(h2, dim_h, dim_out, name='output_layer')\n",
    "    \n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Place holder\n",
    "To train the neural network with mini-batch gradient descent, placeholders should be defined for mini-batch input data and target data. In addition, None type is used so that any batch size of data can be fed into the placeholder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, [None, 784])\n",
    "y = tf.placeholder(tf.float32, [None, 10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct graph\n",
    "Now, we will build a graph for our neural network. Note that you should run the cell below only once. If you run this more than once, an error message will be printed out: `\"ValueError: Variable h1/w already exists, disallowed.\"`. This is because we used `tf.get_variable` above and this function doesn't allow creating variables with the existing names. \n",
    "To fix this problem, you can just type `tf.get_variable_scope().reuse_variables()` before cell below and run it. Then, `tf.get_variable`  will use previously created tensor variables instead of trying to create new ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Construct model with default value\n",
    "out = neural_network(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss, Optimizer and Summary\n",
    "To train the neural network, we should implement loss (this is a tensor) and optimizer (this is an operator). We can use `tf.nn.softmax_cross_entropy_with_logits` function to compute the loss. This function expects unscaled logits, since it performs a softmax on logits internally for efficiency. Do not call this op with the output of softmax, as it will produce incorrect results.\n",
    "Also, we choose `tf.train.RMSPropOptimizer` as our optimizer to minimize the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# loss \n",
    "with tf.name_scope('loss'):\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(out, y))\n",
    "\n",
    "# accuracy\n",
    "with tf.name_scope('accuracy'):\n",
    "    pred = tf.argmax(out, 1)\n",
    "    target = tf.argmax(y, 1)\n",
    "    correct_pred = tf.equal(pred, target)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "    \n",
    "# train op\n",
    "with tf.name_scope('optimizer'):\n",
    "    optimizer = tf.train.RMSPropOptimizer(learning_rate=0.001)\n",
    "    grads = tf.gradients(loss, tf.trainable_variables())\n",
    "    grads_and_vars = list(zip(grads, tf.trainable_variables()))\n",
    "    train_op = optimizer.apply_gradients(grads_and_vars=grads_and_vars)\n",
    "\n",
    "# add summary op   \n",
    "tf.scalar_summary('batch_loss', loss)\n",
    "for var in tf.trainable_variables():\n",
    "    tf.histogram_summary(var.op.name, var)\n",
    "for grad, var in grads_and_vars:\n",
    "    tf.histogram_summary(var.op.name+'/gradient', grad)\n",
    "\n",
    "summary_op = tf.merge_all_summaries() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Session: train and test model\n",
    "From above, we build our neural network model to classify the MNIST dataset. To launch our model, we will implement a session where the model is actually trained and tested on the MNIST dataset.\n",
    "\n",
    "** Train **\n",
    "\n",
    "First, we initialize all variables we created above. \n",
    "This can be done by running [tf.initialize_all_variables()](https://www.tensorflow.org/versions/r0.11/api_docs/python/state_ops.html#initialize_all_variables). All variables in the default graph are initialized with `tf.random_uniform_initializer` in our case.\n",
    "The most important part of code in the training phase is `sess.run([train_op, loss], feed_dict={x: x_batch, y:y_batch})`. This part of code feeds mini-batch data into placeholder and run optimizer to update variables with `tf.train.RMSPropOptimizer` once. \n",
    "Also, loss is evaluated to print out the average loss for each epoch.\n",
    "\n",
    "** Test **\n",
    "\n",
    "Testing phase is quite simple. We use `sess.run(accuracy, {x: mnist.test.images, y: mnist.test.labels})` to print out the test accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.573\n",
      "Epoch 2, Loss: 0.108\n",
      "Epoch 3, Loss: 0.065\n",
      "Epoch 4, Loss: 0.044\n",
      "Epoch 5, Loss: 0.032\n",
      "Epoch 6, Loss: 0.023\n",
      "Epoch 7, Loss: 0.019\n",
      "Epoch 8, Loss: 0.015\n",
      "Epoch 9, Loss: 0.012\n",
      "Epoch 10, Loss: 0.010\n",
      "Epoch 11, Loss: 0.008\n",
      "Epoch 12, Loss: 0.008\n",
      "Epoch 13, Loss: 0.007\n",
      "Epoch 14, Loss: 0.006\n",
      "Epoch 15, Loss: 0.005\n",
      "Finished training!\n",
      "\n",
      "Test accuracy: 0.981\n"
     ]
    }
   ],
   "source": [
    "batch_size = 100\n",
    "log_path = 'log/'\n",
    "\n",
    "# launch the graph\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "with tf.Session(config=config) as sess:\n",
    "    # initialize tensor variables\n",
    "    tf.initialize_all_variables().run()\n",
    "    summary_writer = tf.train.SummaryWriter(log_path, graph=tf.get_default_graph())\n",
    "    # training cycle\n",
    "    for epoch in range(15):\n",
    "        avg_loss = 0.\n",
    "        n_iters_per_epoch = int(mnist.train.num_examples / batch_size)\n",
    "        # loop over all batches\n",
    "        for i in range(n_iters_per_epoch):\n",
    "            x_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "            # run optimization op (backprop) and loss op (to get loss value)\n",
    "            feed_dict={x: x_batch, y: y_batch}\n",
    "            _, c = sess.run([train_op, loss], feed_dict=feed_dict)\n",
    "            # compute average loss\n",
    "            avg_loss += c / n_iters_per_epoch\n",
    "            \n",
    "            if i % 10 == 0:\n",
    "                summary = sess.run(summary_op, feed_dict)\n",
    "                summary_writer.add_summary(summary, epoch*n_iters_per_epoch + i)\n",
    "        print \"Epoch %d, Loss: %.3f\"% (epoch+1, avg_loss)\n",
    "    print \"Finished training!\"\n",
    "    \n",
    "    print \"\"\n",
    "    print \"Test accuracy:\", sess.run(accuracy, {x: mnist.test.images, y: mnist.test.labels})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "## Execute the TensorBoard\n",
    "To execute the tensorboard, open the new terminal, run command below and open http://localhost:6005/ into your web browser.\n",
    "```bash\n",
    "$ tensorboard --logdir='./log' --port=6005 \n",
    "```"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
