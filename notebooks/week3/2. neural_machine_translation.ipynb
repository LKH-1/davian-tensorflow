{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Machine Translation\n",
    "In this tutorial, we are going to implement a neural machine translator for translating English to Korean. Our model is motivated by [this paper](https://arxiv.org/abs/1409.0473).\n",
    "\n",
    "\n",
    "![alt text](jpg/nmt2.jpg \"model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "\n",
    "from utils import preprocess, decode_sequence\n",
    "from ops import lstm_cell, rnn_encoder, attention_mechanism\n",
    "from ops import rnn_decoder, rnn_decoder_test\n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dog ordinarily remains loyal to a considerate master\n",
      "개는 보통 사려 깊은 주인에게 충실합니다\n",
      "\n",
      "Class distinctions between people have no part in a dog's life\n",
      "사람들 사이의 계급 구분은 개의 삶에 아무런 영향을 미치지 않습니다\n",
      "\n",
      "It can be a faithful companion to either rich or poor\n",
      "부자 나 가난한 자에게 충실한 동반자가 될 수 있습니다\n",
      "\n"
     ]
    }
   ],
   "source": [
    "source_sequences = open('data/source.txt').readlines()\n",
    "target_sequences = open('data/target.txt').readlines()\n",
    "\n",
    "k = 3\n",
    "for eng, kor in zip(source_sequences[0:k], target_sequences[0:k]):\n",
    "    print eng, kor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "source, source_mask, word_to_idx_src, source_seq_length, source_vocab_size = preprocess(source_sequences)\n",
    "target, target_mask, word_to_idx_trg, target_seq_length, target_vocab_size = preprocess(target_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the dog ordinarily remains loyal to a considerate master .\n",
      "개는 보통 사려 깊은 주인에게 충실합니다 .\n"
     ]
    }
   ],
   "source": [
    "eng = decode_sequence(source[0:1], word_to_idx_src)\n",
    "kor =  decode_sequence(target[0:1], word_to_idx_trg)\n",
    "\n",
    "for e, k in zip(eng, kor):\n",
    "    print e\n",
    "    print k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 2\n",
    "dim_h = 128\n",
    "dim_emb = 64 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params = {}\n",
    "\n",
    "# weights and biases for rnn encoder \n",
    "params['w_encoder'] = tf.Variable(tf.random_normal(shape=[dim_h + dim_emb, dim_h*4], stddev=0.1), name='w_encoder')\n",
    "params['b_encoder'] = tf.Variable(tf.zeros(shape=[dim_h*4]), name='b_encoder')\n",
    "\n",
    "# weights and biases for rnn decoder\n",
    "params['w_decoder'] = tf.Variable(tf.random_normal(shape=[dim_h*2 + dim_emb, dim_h*4], stddev=0.1), name='w_decoder')\n",
    "params['b_decoder'] = tf.Variable(tf.zeros(shape=[dim_h*4]), name='w_decoder')\n",
    "\n",
    "# weigths and biases for attention mechanism\n",
    "params['w1_att'] = tf.Variable(tf.random_normal(shape=[dim_h, dim_h], stddev=0.1), name='w1_att')\n",
    "params['w2_att'] = tf.Variable(tf.random_normal(shape=[dim_h, dim_h], stddev=0.1), name='w2_att')\n",
    "params['b_att'] = tf.Variable(tf.zeros(shape=[dim_h]), name='b_att')\n",
    "params['w3_att'] = tf.Variable(tf.random_normal(shape=[dim_h, 1], stddev=0.1), name='w3_att')\n",
    "\n",
    "# embedding matrices for source and target languages\n",
    "params['w_emb_src'] = tf.Variable(tf.random_uniform(shape=[source_vocab_size, dim_emb], minval=-1.0, maxval=1.0),\n",
    "                                 name='w_emb_src')\n",
    "params['w_emb_trg'] = tf.Variable(tf.random_uniform(shape=[target_vocab_size, dim_emb], minval=-1.0, maxval=1.0),\n",
    "                                 name='w_emb_trg')\n",
    "\n",
    "\n",
    "# weigths and biases for initializing initial cell and hidden state in decoder\n",
    "params['w_init_c'] = tf.Variable(tf.random_normal(shape=[dim_h, dim_h], stddev=0.1), name='w_init_c')\n",
    "params['b_init_c'] = tf.Variable(tf.zeros(shape=[dim_h]), name='b_init_c')\n",
    "params['w_init_h'] = tf.Variable(tf.random_normal(shape=[dim_h, dim_h], stddev=0.1), name='w_init_h')\n",
    "params['b_init_h'] = tf.Variable(tf.zeros(shape=[dim_h]), name='b_init_h')\n",
    "\n",
    "# weights and biases for computing logits (include softmax layer)\n",
    "params['w1_logit'] = tf.Variable(tf.random_normal(shape=[dim_h, dim_h], stddev=0.1), name='w1_logit')\n",
    "params['b1_logit'] = tf.Variable(tf.zeros(shape=[dim_h]), name='b1_logit')\n",
    "params['w2_logit'] = tf.Variable(tf.random_normal(shape=[dim_h, target_vocab_size], stddev=0.1), name='w2_logit')\n",
    "params['b2_logit'] = tf.Variable(tf.zeros(shape=[target_vocab_size]), name='b2_logit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf_source = tf.placeholder(dtype=tf.int64, shape=[None, source_seq_length], name='source_seq')\n",
    "tf_source_mask = tf.placeholder(dtype=tf.int64, shape=[None, source_seq_length], name='source_mask')\n",
    "tf_target = tf.placeholder(dtype=tf.int64, shape=[None, target_seq_length], name='target_seq')\n",
    "tf_target_mask = tf.placeholder(dtype=tf.int64, shape=[None, target_seq_length], name='target_mask')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# encoder\n",
    "h_encoded = rnn_encoder(tf_source, params) # (batch_size, source_seq_length, dim_h)\n",
    "\n",
    "# decoder (train mode)\n",
    "loss = rnn_decoder(tf_target, h_encoded, tf_source_mask, tf_target_mask, params)\n",
    "\n",
    "# decoder (test mode)\n",
    "sampled_seq = rnn_decoder_test(h_encoded, tf_source_mask, word_to_idx_trg, params) # (batch_size, target_seq_length-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train op\n",
    "with tf.name_scope('optimizer'):\n",
    "    optimizer = tf.train.RMSPropOptimizer(learning_rate=0.001)\n",
    "    grads = tf.gradients(loss, tf.trainable_variables())\n",
    "    grads_and_vars = list(zip(grads, tf.trainable_variables()))\n",
    "    train_op = optimizer.apply_gradients(grads_and_vars=grads_and_vars)\n",
    "\n",
    "# add summary op   \n",
    "tf.scalar_summary('batch_loss', loss)\n",
    "for var in tf.trainable_variables():\n",
    "    tf.histogram_summary(var.op.name, var)\n",
    "for grad, var in grads_and_vars:\n",
    "    tf.histogram_summary(var.op.name+'/gradient', grad)\n",
    "\n",
    "summary_op = tf.merge_all_summaries() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "loss at epoch 10: 95.098\n",
      "원문: here i was in contact with what i most wanted, life in the raw .\n",
      "번역: 그것은 그것은 그것은 <PAD> .\n",
      "\n",
      "loss at epoch 20: 52.845\n",
      "원문: dogs have endeared themselves to many over the years .\n",
      "번역: 그것은 놀라운 이야기였습니다, 이야기였습니다, 그것은 관심을 .\n",
      "\n",
      "loss at epoch 30: 23.770\n",
      "원문: it was an amazing story, and it piqued my interest .\n",
      "번역: 그것은 놀라운 이야기였습니다, 그리고 그것은 내 관심을 .\n",
      "\n",
      "loss at epoch 40: 10.007\n",
      "원문: dogs have endeared themselves to many over the years .\n",
      "번역: 개들은 수년에 걸쳐 많은 사람들에게 사랑 받아 왔습니다 .\n",
      "\n",
      "loss at epoch 50: 2.791\n",
      "원문: it was an amazing story, and it piqued my interest .\n",
      "번역: 그것은 놀라운 이야기였습니다, 그리고 그것은 내 관심을 자극했습니다 .\n",
      "\n",
      "loss at epoch 60: 0.788\n",
      "원문: it was an amazing story, and it piqued my interest .\n",
      "번역: 그것은 놀라운 이야기였습니다, 그리고 그것은 내 관심을 자극했습니다 .\n",
      "\n",
      "loss at epoch 70: 0.187\n",
      "원문: in those three years i must have witnessed pretty well every emotion of which man is capable .\n",
      "번역: 그 3 년 동안 나는 인간이 할 수있는 모든 감정을 꽤 잘 목격했을 것입니다 .\n",
      "\n",
      "loss at epoch 80: 0.043\n",
      "원문: here i was in contact with what i most wanted, life in the raw .\n",
      "번역: 여기에서 나는 내가 가장 원했던 것과 연락을 취했다 .\n",
      "\n",
      "loss at epoch 90: 0.011\n",
      "원문: it excited the novelist in me .\n",
      "번역: 그것은 저의 소설가를 흥분 시켰습니다 .\n",
      "\n",
      "loss at epoch 100: 0.004\n",
      "원문: it was an amazing story, and it piqued my interest .\n",
      "번역: 그것은 놀라운 이야기였습니다, 그리고 그것은 내 관심을 자극했습니다 .\n"
     ]
    }
   ],
   "source": [
    "num_epoch = 100\n",
    "tot_batch = len(source)\n",
    "num_iter_per_epoch = int(np.ceil(tot_batch / batch_size)) \n",
    "log_path = 'log/'\n",
    "print_every = 10\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True \n",
    "with tf.Session(config=config) as sess:\n",
    "    tf.initialize_all_variables().run()\n",
    "    summary_writer = tf.train.SummaryWriter(log_path, graph=tf.get_default_graph())\n",
    "    for e in range(num_epoch):\n",
    "        # TODO: random shuffle train dataset\n",
    "        for i in range(num_iter_per_epoch):\n",
    "            # get batch data\n",
    "            source_batch = source[i*batch_size:(i+1)*batch_size, :]\n",
    "            source_mask_batch = source_mask[i*batch_size:(i+1)*batch_size, :]\n",
    "            target_batch = target[i*batch_size:(i+1)*batch_size, :]\n",
    "            target_mask_batch = target_mask[i*batch_size:(i+1)*batch_size, :]\n",
    "            \n",
    "            # train op\n",
    "            feed_dict={tf_source: source_batch, tf_source_mask: source_mask_batch, \n",
    "                       tf_target: target_batch, tf_target_mask: target_mask_batch}\n",
    "            _, l = sess.run(fetches=[train_op, loss], feed_dict=feed_dict)\n",
    "            \n",
    "            if i % 5 == 0:\n",
    "                summary = sess.run(summary_op, feed_dict)\n",
    "                summary_writer.add_summary(summary, e*num_iter_per_epoch + i)\n",
    "                \n",
    "        # print sampled sequences\n",
    "        if (e+1) % print_every == 0:\n",
    "            print (\"\\nloss at epoch %d: %.3f\" %(e+1, l))\n",
    "            \n",
    "            feed_dict={tf_source: source, tf_source_mask: source_mask}\n",
    "            np_sampled_seq = sess.run(fetches=sampled_seq, feed_dict=feed_dict) \n",
    "            \n",
    "            eng = decode_sequence(sequences=source, word_to_idx=word_to_idx_src)\n",
    "            kor = decode_sequence(sequences=np_sampled_seq, word_to_idx=word_to_idx_trg)\n",
    "            \n",
    "            rand_idx = np.random.randint(tot_batch)\n",
    "            print '원문: %s' %eng[rand_idx]\n",
    "            print '번역: %s' %kor[rand_idx]"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
