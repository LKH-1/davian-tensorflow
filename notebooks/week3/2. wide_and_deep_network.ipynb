{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wide and Deep Network\n",
    "In this tutorial, we are going to implement a [Wide and Deep Network](https://arxiv.org/abs/1606.07792) to solve a classification problem. A Wide and Deep Network combines a linear model with a feed forward neural net so that our predictions will have memorization and generalization. This type of model can be used for classification and regression problems. This allows for less feature engineering with relatively accurate predictions. Thus, getting the best of both worlds.\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "![alt text](jpg/wide_and_deep_model.jpg \"model image\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Data\n",
    "We are going to be using the Titanic Kaggle data to predict whether or not the passenger will survive based on certain attributes like Name, Gender, what ticket they had, the fare they paid the cabin they stayed in etc. For more information on this data set check out here at [Kaggle](https://www.kaggle.com/c/titanic/data).\n",
    "\n",
    "\n",
    "First off we’re going to define all of our columns as Continuous or Categorical.\n",
    "\n",
    "<b>Continuous columns </b>— any numerical value in a continuous range. Pretty much if it is a numerical representation like money, or age.\n",
    "\n",
    "<b>Categorical columns </b>— part of a finite set. Like male or female, or even what country someone is from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "combiner='sum' # combiner다른 거 해보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "CATEGORICAL_COLUMNS = [\"Name\", \"Gender\", \"Embarked\", \"Cabin\"]\n",
    "CONTINUOUS_COLUMNS = [\"Age\", \"SibSp\", \"Parch\", \"Fare\", \"PassengerId\", \"Pclass\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are only looking to see if a person survived, this is a binary classification problem. We predict a 1 if that person survives and a 0… if they do not :( , We then create a column solely for our survived category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SURVIVED_COLUMN = \"Survived\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Network\n",
    "Now we can get to creating the columns and adding embedding layers. When we build our model were going to want to change our categorical columns into a sparse column. For our columns with a small set of categories such as Gender or Embarked (C, Q or S) we will transform them into sparse columns with keys. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gender = tf.contrib.layers.sparse_column_with_keys(column_name='Gender', keys=['female', 'male'], combiner=combiner)\n",
    "embarked = tf.contrib.layers.sparse_column_with_keys(\n",
    "    column_name='Embarked',keys=['C', 'Q', 'S'], combiner='sum') # Port of Embarkation (C: Cherbourg; Q: Queenstown; S: Southampton)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The other categorical columns have many more options than we want to put keys, and since we don’t have a vocab file to map all of the possible categories into an integer we will hash them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cabin = tf.contrib.layers.sparse_column_with_hash_bucket('Cabin', hash_bucket_size=1000, combiner=combiner)\n",
    "name = tf.contrib.layers.sparse_column_with_hash_bucket('Name', hash_bucket_size=1000, combiner=combiner)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our continuous columns we want to use their real value. The reason that passenger id is in continuous and not categorical is because they’re not in string format and they’re already an integer ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "age = tf.contrib.layers.real_valued_column(\"Age\")\n",
    "passenger_id = tf.contrib.layers.real_valued_column(\"PassengerId\")\n",
    "sib_sp = tf.contrib.layers.real_valued_column(\"SibSp\")  # Number of Siblings/Spouses Aboard\n",
    "parch = tf.contrib.layers.real_valued_column(\"Parch\")   # Number of Parents/Children Aboard\n",
    "fare = tf.contrib.layers.real_valued_column(\"Fare\")     # Passenger Fare\n",
    "p_class = tf.contrib.layers.real_valued_column(\"Pclass\") # Passenger Class (1: 1st; 2: 2nd; 3: 3rd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to bucket the ages. Bucketization allows us to find the survival correlation by certain age groups and not by all the ages as a whole, thus increasing our accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "age_buckets = tf.contrib.layers.bucketized_column(age, boundaries=[5, 18, 25,30, 35, 40,45, 50, 55, 65])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Almost done, we are going to define our wide columns and our deep columns. Our wide columns are going to effectively memorize interactions between our features. Our wide columns don’t generalize our features, this is why we have our deep columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wide_columns = [gender, embarked, cabin, name, age_buckets,\n",
    "                  tf.contrib.layers.crossed_column([age_buckets, gender], hash_bucket_size=int(1e6), combiner=combiner),\n",
    "                  tf.contrib.layers.crossed_column([embarked, name], hash_bucket_size=int(1e4), combiner=combiner)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#wide_columns = [gender, embarked, p_class, cabin, name, age_buckets,\n",
    "#                  tf.contrib.layers.crossed_column([p_class, cabin], hash_bucket_size=int(1e4), combiner='sum'), 이거 쓰면 에러\n",
    "#                  tf.contrib.layers.crossed_column([age_buckets, gender], hash_bucket_size=int(1e6), combiner='sum'),\n",
    "#                  tf.contrib.layers.crossed_column([embarked, name], hash_bucket_size=int(1e4), combiner='sum')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The benefit of having these deep columns is that it takes our sparse high dimension features and reduces them into low dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "deep_columns = [age, passenger_id, sib_sp, parch, fare, p_class,\n",
    "                  tf.contrib.layers.embedding_column(gender, dimension=8, combiner=combiner),\n",
    "                  tf.contrib.layers.embedding_column(embarked, dimension=8, combiner=combiner),\n",
    "                  tf.contrib.layers.embedding_column(cabin, dimension=8, combiner=combiner),\n",
    "                  tf.contrib.layers.embedding_column(name, dimension=8, combiner=combiner)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We finish off our function by creating our classifier with our deep columns and wide columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using default config.\n"
     ]
    }
   ],
   "source": [
    "model = tf.contrib.learn.DNNLinearCombinedClassifier(linear_feature_columns=wide_columns,\n",
    "                                                          dnn_feature_columns=deep_columns,\n",
    "                                                          dnn_hidden_units=[100, 50],\n",
    "                                                          enable_centered_bias=False,\n",
    "                                                          model_dir='model/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Evaluate the Model\n",
    "\n",
    "The last thing we will have to do before running the network is create mappings for our continuous and categorical columns. What we are doing here by creating this function, and this is standard throughout the Tensorflow learning code, is creating an input function for our dataframe. This converts our dataframe into something that Tensorflow can manipulate. The benefit of this is that we can change and tweak how our tensors are being created. If we wanted we could pass feature columns into .fit .feature .predict as an individually created column like we have above with our features, but this is a much cleaner solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def input_fn(df, train=False):\n",
    "    \"\"\"Input builder function.\"\"\"\n",
    "    # Creates a dictionary mapping from each continuous feature column name (k) to\n",
    "    # the values of that column stored in a constant Tensor.\n",
    "    continuous_cols = {k: tf.constant(df[k].values) for k in CONTINUOUS_COLUMNS}\n",
    "    # Creates a dictionary mapping from each categorical feature column name (k)\n",
    "    # to the values of that column stored in a tf.SparseTensor.\n",
    "    categorical_cols = {k: tf.SparseTensor(\n",
    "        indices=[[i, 0] for i in range(df[k].size)],\n",
    "        values=df[k].values,\n",
    "        shape=[df[k].size, 1]) for k in CATEGORICAL_COLUMNS}\n",
    "    # Merges the two dictionaries into one.\n",
    "    feature_cols = dict(continuous_cols)\n",
    "    feature_cols.update(categorical_cols)\n",
    "    # Converts the label column into a constant Tensor.\n",
    "    if train:\n",
    "        label = tf.constant(df[SURVIVED_COLUMN].values)\n",
    "        # Returns the feature columns and the label.\n",
    "        return feature_cols, label\n",
    "    else:\n",
    "        return feature_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now after all this we can write our training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_and_eval(model):\n",
    "    \"\"\"Train and evaluate the model.\"\"\"\n",
    "    df_train = pd.read_csv(tf.gfile.Open(\"./data/train.csv\"), skipinitialspace=True, engine='python')\n",
    "    df_test = pd.read_csv(tf.gfile.Open(\"./data/test.csv\"), skipinitialspace=True, engine='python')\n",
    "    model.fit(input_fn=lambda: input_fn(df_train, True), steps=300)\n",
    "    print model.predict(input_fn=lambda: input_fn(df_test), as_iterable=True)\n",
    "    results = model.evaluate(input_fn=lambda: input_fn(df_train, True), steps=1)\n",
    "    for key in sorted(results):\n",
    "        print(\"%s: %s\" % (key, results[key]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We read in our csv files that were preprocessed, like effectively imputed missing values, for simplicity sake.\n",
    "These csv’s are converted to tensors using our input_fn by lambda. we build our estimator then we print our predictions and print out our evaluation results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Given features: {'Fare': <tf.Tensor 'Const_3:0' shape=(891,) dtype=float64>, 'Cabin': <tensorflow.python.framework.ops.SparseTensor object at 0x7f32483cbe90>, 'SibSp': <tf.Tensor 'Const_1:0' shape=(891,) dtype=int64>, 'Name': <tensorflow.python.framework.ops.SparseTensor object at 0x7f3248391dd0>, 'Embarked': <tensorflow.python.framework.ops.SparseTensor object at 0x7f32482e8dd0>, 'Gender': <tensorflow.python.framework.ops.SparseTensor object at 0x7f339c0a0f90>, 'Age': <tf.Tensor 'Const:0' shape=(891,) dtype=int64>, 'PassengerId': <tf.Tensor 'Const_4:0' shape=(891,) dtype=int64>, 'Parch': <tf.Tensor 'Const_2:0' shape=(891,) dtype=int64>, 'Pclass': <tf.Tensor 'Const_5:0' shape=(891,) dtype=int64>}, required signatures: {'Fare': TensorSignature(dtype=tf.float64, shape=TensorShape([Dimension(891)]), is_sparse=False), 'Name': TensorSignature(dtype=tf.string, shape=None, is_sparse=True), 'Embarked': TensorSignature(dtype=tf.string, shape=None, is_sparse=True), 'Gender': TensorSignature(dtype=tf.string, shape=None, is_sparse=True), 'Age': TensorSignature(dtype=tf.int64, shape=TensorShape([Dimension(891)]), is_sparse=False), 'Parch': TensorSignature(dtype=tf.int64, shape=TensorShape([Dimension(891)]), is_sparse=False), 'Pclass': TensorSignature(dtype=tf.int64, shape=TensorShape([Dimension(891)]), is_sparse=False), 'SibSp': TensorSignature(dtype=tf.int64, shape=TensorShape([Dimension(891)]), is_sparse=False), 'PassengerId': TensorSignature(dtype=tf.int64, shape=TensorShape([Dimension(891)]), is_sparse=False), 'Cabin': TensorSignature(dtype=tf.string, shape=None, is_sparse=True)}.\n",
      "WARNING:tensorflow:Given targets: Tensor(\"Const_6:0\", shape=(891,), dtype=int64), required signatures: TensorSignature(dtype=tf.int64, shape=TensorShape([Dimension(891)]), is_sparse=False).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<generator object <genexpr> at 0x7f32bc29aeb0>\n",
      "accuracy: 0.866442\n",
      "accuracy/baseline_target_mean: 0.383838\n",
      "accuracy/threshold_0.500000_mean: 0.866442\n",
      "auc: 0.952788\n",
      "global_step: 600\n",
      "labels/actual_target_mean: 0.383838\n",
      "labels/prediction_mean: 0.391919\n",
      "loss: 0.316167\n",
      "precision/positive_threshold_0.500000_mean: 0.830861\n",
      "recall/positive_threshold_0.500000_mean: 0.818713\n"
     ]
    }
   ],
   "source": [
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True   \n",
    "with tf.Session(config=config) as sess:\n",
    "    train_and_eval(model)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
