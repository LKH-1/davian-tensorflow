import tensorflow as tf


def lstm_cell(x, c_prev, h_prev, w, b, ctx=None):
    """Forward propagate LSTM cell for 1 time step
    
    1) lstm_cell used in encoder parts: do not use ctx(context vector generated by attention mechanism)
    2) lstm_cell used in decoder parts: use ctx for additional input
    
    Args:
        x: input at current time of shape (batch_size, dim_emb)
        h_prev: previous hidden state of shape (batch_size, dim_h)
        c_prev: previous cell state of shape (batch_size, dim_h)
        w: linear transform weights of shape (dim_emb + dim_h ((optional)+ dim_h), dim_h x 4)
        b: biases of shape (dim_h x 4)
        ctx: (optional) context vectors generated by attention mechanism of shape (batct_size, dim_h)
        
    Returns:
        c: current cell state
        h: current hidden state
    """
    # concatenate input and previous hidden state for efficient computation
    if ctx is not None:
        inputs = tf.concat(concat_dim=1, values=[x, h_prev, ctx])
    else:
        inputs = tf.concat(concat_dim=1, values=[x, h_prev])
    
    # compute gates and new input
    a = tf.matmul(inputs, w) + b
    i, o, f, g = tf.split(split_dim=1, num_split=4, value=a)
    i = tf.nn.sigmoid(i)
    f = tf.nn.sigmoid(o)
    o = tf.nn.sigmoid(f)
    g = tf.nn.tanh(g)
    
    # compute current cell and hidden state
    c = f * c_prev + i * g
    h = o * tf.nn.tanh(c)
    
    return c, h


def rnn_encoder(source, params):
    """Compute encoder's hidden states for entire time step.
    
    Args:
        source: source sequences of shape (batch_size, source_seq_length)
        params: weights and biases
    
    Returns:
        h_encoded: hidden states encoded by rnn encoder of shape (batch_size, source_seq_length, dim_h)
    """
    source_seq_length = source.get_shape().as_list()[1]
    dim_h = tf.shape(params['w_encoder'])[1] / 4
    
    # word embedding of shape (batch_size, source_seq_length, dim_emb)
    word_emb = tf.nn.embedding_lookup(params['w_emb_src'], source)
    
    
    h_list = []
    for t in range(source_seq_length):
        if t == 0:
            c = tf.zeros(shape=[tf.shape(source)[0], dim_h])
            h = tf.zeros(shape=[tf.shape(source)[0], dim_h])
            
        # lstm step forward 
        # TODO: upgrade to bidirectional rnn 
        c, h = lstm_cell(word_emb[:, t, :], c, h, params['w_encoder'], params['b_encoder'])
        h_list.append(h)
    
    # encoder hidden states for entire time step
    h_encoded = tf.pack(values=h_list, axis=1)
    
    return h_encoded


def attention_mechanism(h_encoded, h_prev_decoded, mask, w1, w2, b, w3):
    """Select adaptively the relevant part of the encoder's hidden states.
    
    Args:
        h_encoded: hidden states encoded by rnn encoder of shape (batch_size, source_seq_length, dim_h)
        h_prev_decoded: previous hidden states of rnn decoder of shape (batch_size, dim_h)
        mask: mask indicating whether encoder's hidden state is valid or not of shape (batch_size, source_seq_length)
        w1: linear transform weights of shape (dim_h, dim_h)
        w2: linear transform weights of shape (dim_h, dim_h)
        b: biases of shape (dim_h)
        w3: linear transform for hidden layer of shape (dim_h, 1)
    
    Returns:
        context: context vectors of shape (batch_size, dim_h)
        alpha: attention weigths of shape (batch_size, source_seq_length)
    """
    
    source_seq_length = tf.shape(h_encoded)[1]
    dim_h = tf.shape(h_encoded)[2]
    
    h_encoded = tf.reshape(h_encoded, shape=[-1, dim_h])
    h_encoded = tf.reshape(tf.matmul(h_encoded, w1), [-1, source_seq_length, dim_h])
    
    h = tf.nn.relu(h_encoded + tf.expand_dims(tf.matmul(h_prev_decoded, w2), 1) + b)  # note for broadcasting
    
    h = tf.reshape(h, [-1, dim_h])
    out = tf.reshape(tf.matmul(h, w3), [-1, source_seq_length]) 
    alpha = tf.nn.softmax(out * tf.cast(mask, dtype=tf.float32), name='attention_weights')  
    
    context = h_encoded * tf.expand_dims(alpha, dim=2)  
    context = tf.reduce_sum(context, reduction_indices=1)
    
    return context

def rnn_decoder(target, h_encoded, source_mask, target_mask, params):
    """Compute decoder's hidden state and return loss 
    
    Args:
        target: target sequences of shape (batch_size, target_seq_length)
        h_encoded: hidden states encoded by rnn encoder of shape (batch_size, source_seq_length, dim_h)
        source_mask: mask for source sequences of shape (batch_size, source_seq_length)
        target_mask: mask for target sequences of shape (batch_size, target_seq_length)
        params: weights and biases
    Returns:
        loss: (mini-batch) softmax cross entropy loss 
    """
    target_seq_length = target.get_shape().as_list()[1]
    
    
    target_in = target[:, :-1]
    target_out = target[:, 1:]
    
    # word embedding 
    word_emb = tf.nn.embedding_lookup(params['w_emb_trg'], target_in)
    
    
    loss = 0.0
    for t in range(target_seq_length - 1):
        if t == 0:
            h_encoded_mean = tf.reduce_mean(h_encoded, reduction_indices=1)    
            # compute initial cell and hidden states
            c = tf.nn.tanh(tf.matmul(h_encoded_mean, params['w_init_c']) + params['b_init_c'])
            h = tf.nn.tanh(tf.matmul(h_encoded_mean, params['w_init_h']) + params['b_init_h'])
        
        # compute context vector 
        context = attention_mechanism(h_encoded, 
                                      h, 
                                      source_mask,
                                      params['w1_att'], 
                                      params['w2_att'], 
                                      params['b_att'], 
                                      params['w3_att'])
        
        # decoder lstm step forward
        c, h = lstm_cell(word_emb[:,t,:], c, h, params['w_decoder'], params['b_decoder'], ctx=context)
    
        # compute softmax layer
        h_logit = tf.nn.tanh(tf.matmul(h, params['w1_logit']) + params['b1_logit'])
        out = tf.matmul(h_logit, params['w2_logit']) + params['b2_logit'] # (batch_size, target_vocab_size)
        loss += tf.reduce_sum(
            tf.nn.sparse_softmax_cross_entropy_with_logits(
                out, target_out[:, t]) * tf.cast(target_mask[:, t], dtype=tf.float32))
    
    return loss

def rnn_decoder_test(h_encoded, source_mask, word_to_idx, params):
    """Decoder for 'test' mode. sample the sequences by greedy search 
    
    Args:
        h_encoded: hidden states encoded by rnn encoder of shape (batch_size, source_seq_length, dim_h)
        source_mask: mask for source sequences of shape (batch_size, source_seq_length)
        params: weights and biases
        word_to_idx: word-to-index mapping dictionary for target sequences
    Returns:
        sampled_seq: sampled word indices of shape (batch_size, target_seq_length - 1)
    """
    # hyper parameter
    max_length = 30 
    
    loss = 0.0
    sampled_word_list = []
    for t in range(max_length):
        if t == 0:
            # compute initial cell and hidden states
            h_encoded_mean = tf.reduce_mean(h_encoded, reduction_indices=1)    
            c = tf.nn.tanh(tf.matmul(h_encoded_mean, params['w_init_c']) + params['b_init_c'])
            h = tf.nn.tanh(tf.matmul(h_encoded_mean, params['w_init_h']) + params['b_init_h'])
            
            # initial word embedding with 'start' token
            word_emb = tf.nn.embedding_lookup(params['w_emb_trg'], 
                                              tf.fill(dims=[tf.shape(h_encoded)[0]], 
                                                      value=word_to_idx['<START>']))
        else:
            # word embedding with previously sampled word
            word_emb = tf.nn.embedding_lookup(params['w_emb_trg'], sampled_word) # (batch_size, dim_emb)
        
        
        # compute context vector 
        context = attention_mechanism(h_encoded, 
                                      h, 
                                      source_mask,
                                      params['w1_att'], 
                                      params['w2_att'], 
                                      params['b_att'], 
                                      params['w3_att'])
        
        # decoder lstm step forward
        c, h = lstm_cell(word_emb, c, h, params['w_decoder'], params['b_decoder'], ctx=context)
    
        # compute softmax layer
        h_logit = tf.nn.tanh(tf.matmul(h, params['w1_logit']) + params['b1_logit'])
        out = tf.matmul(h_logit, params['w2_logit']) + params['b2_logit']  # (batch_size, target_vocab_size)
        sampled_word = tf.arg_max(out, dimension=1)  # (batch_size)
        sampled_word_list.append(sampled_word) # (num_time_step) @ (batch_size)
    
    sampled_seq = tf.transpose(tf.pack(sampled_word_list), (1, 0))
    
    return sampled_seq


